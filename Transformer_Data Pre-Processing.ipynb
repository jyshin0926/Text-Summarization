{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Data Pre-Processing.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEduadycYcS_","executionInfo":{"status":"ok","timestamp":1616558327136,"user_tz":-540,"elapsed":20367,"user":{"displayName":"jaeyoung shin","photoUrl":"https://lh4.googleusercontent.com/-7ZQ3GuRm24k/AAAAAAAAAAI/AAAAAAAAAO0/eUnHI3HOU-w/s64/photo.jpg","userId":"04984017837842544937"}},"outputId":"ede7eb32-1b77-415a-f2b5-f6ef9941411e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlWnYPTxYqdb","executionInfo":{"status":"ok","timestamp":1616558360798,"user_tz":-540,"elapsed":606,"user":{"displayName":"jaeyoung shin","photoUrl":"https://lh4.googleusercontent.com/-7ZQ3GuRm24k/AAAAAAAAAAI/AAAAAAAAAO0/eUnHI3HOU-w/s64/photo.jpg","userId":"04984017837842544937"}},"outputId":"37c87cb6-0797-4829-cebb-5968369be2ac"},"source":["%cd /content/drive/MyDrive/puzzleAI/Transformer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/puzzleAI/Transformer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbagBr3YYacR","executionInfo":{"status":"ok","timestamp":1616558537979,"user_tz":-540,"elapsed":171441,"user":{"displayName":"jaeyoung shin","photoUrl":"https://lh4.googleusercontent.com/-7ZQ3GuRm24k/AAAAAAAAAAI/AAAAAAAAAO0/eUnHI3HOU-w/s64/photo.jpg","userId":"04984017837842544937"}},"outputId":"ae2068d6-5df5-4179-e7f7-eb6de8e14c30"},"source":["import numpy as np\n","\n","\n","filename = 'glove.840B.300d.txt' \n","# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n","\n","word_vec_dim = 300 # word_vec_dim = dimension of each word vectors\n","\n","def loadEmbeddings(filename):\n","    vocab2embd = {}\n","\n","    with open(filename) as infile:\n","        for line in infile:\n","            row = line.strip().split(' ')\n","            word = row[0].lower()\n","            # print(word)\n","            if word not in vocab2embd:\n","                vec = np.asarray(row[1:], np.float32)\n","                if len(vec) == word_vec_dim:\n","                    vocab2embd[word] = vec\n","\n","    print('Embedding Loaded.')\n","        \n","    return vocab2embd\n","\n","# Pre-trained word embedding\n","vocab2embd = loadEmbeddings(filename)\n","\n","\n","\n","vocab2embd['<UNK>'] = np.random.randn(word_vec_dim)\n","vocab2embd['<GO>'] = np.random.randn(word_vec_dim)\n","vocab2embd['<PRED>'] = np.random.randn(word_vec_dim)\n","vocab2embd['<EOS>'] = np.random.randn(word_vec_dim)\n","vocab2embd['<PAD>'] = np.zeros(word_vec_dim)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Embedding Loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_E0iWbMYacY","executionInfo":{"status":"ok","timestamp":1616559983879,"user_tz":-540,"elapsed":532991,"user":{"displayName":"jaeyoung shin","photoUrl":"https://lh4.googleusercontent.com/-7ZQ3GuRm24k/AAAAAAAAAAI/AAAAAAAAAO0/eUnHI3HOU-w/s64/photo.jpg","userId":"04984017837842544937"}},"outputId":"95d8955d-9397-4e34-85eb-2cf8d7e7d819"},"source":["import csv\n","import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","import string\n","\n","summaries = []\n","texts = []\n","\n","def clean(text):\n","    text = text.lower()\n","    printable = set(string.printable)\n","    text = \"\".join(list(filter(lambda x: x in printable, text))) #filter funny characters, if any.\n","    return text\n","\n","counter={}\n","max_len_text = 100\n","max_len_sum = 20\n","\n","#max_data = 100000\n","i=0\n","with open('Reviews.csv', 'rt') as csvfile: #Data from https://www.kaggle.com/snap/amazon-fine-food-reviews\n","    Reviews = csv.DictReader(csvfile)\n","    count=0\n","    for row in Reviews:\n","        #if count<max_data:\n","        clean_text = word_tokenize(clean(row['Text']))\n","        clean_summary = word_tokenize(clean(row['Summary']))\n","        \n","        if len(clean_text) <= max_len_text and len(clean_summary) <= max_len_sum:\n","            \n","            for word in clean_text:\n","                if word in vocab2embd:\n","                    counter[word]=counter.get(word,0)+1\n","            for word in clean_summary:\n","                if word in vocab2embd:\n","                    counter[word]=counter.get(word,0)+1\n","\n","            summaries.append(clean_summary)\n","            texts.append(clean_text)\n","        #count+=1\n","        if i%10000==0:\n","            print(\"Processing data {}\".format(i))\n","        i+=1\n","        \n","print(\"Current size of data: \"+str(len(texts)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Processing data 0\n","Processing data 10000\n","Processing data 20000\n","Processing data 30000\n","Processing data 40000\n","Processing data 50000\n","Processing data 60000\n","Processing data 70000\n","Processing data 80000\n","Processing data 90000\n","Processing data 100000\n","Processing data 110000\n","Processing data 120000\n","Processing data 130000\n","Processing data 140000\n","Processing data 150000\n","Processing data 160000\n","Processing data 170000\n","Processing data 180000\n","Processing data 190000\n","Processing data 200000\n","Processing data 210000\n","Processing data 220000\n","Processing data 230000\n","Processing data 240000\n","Processing data 250000\n","Processing data 260000\n","Processing data 270000\n","Processing data 280000\n","Processing data 290000\n","Processing data 300000\n","Processing data 310000\n","Processing data 320000\n","Processing data 330000\n","Processing data 340000\n","Processing data 350000\n","Processing data 360000\n","Processing data 370000\n","Processing data 380000\n","Processing data 390000\n","Processing data 400000\n","Processing data 410000\n","Processing data 420000\n","Processing data 430000\n","Processing data 440000\n","Processing data 450000\n","Processing data 460000\n","Processing data 470000\n","Processing data 480000\n","Processing data 490000\n","Processing data 500000\n","Processing data 510000\n","Processing data 520000\n","Processing data 530000\n","Processing data 540000\n","Processing data 550000\n","Processing data 560000\n","Current size of data: 394240\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jS-cQ7NuYacY"},"source":["vocab = [word for word in counter]\n","counts = [counter[word] for word in vocab]\n","sorted_idx = sorted(range(len(counts)), key=counts.__getitem__)\n","sorted_idx.reverse()\n","vocab = [vocab[idx] for idx in sorted_idx]\n","special_tags = [\"<UNK>\",\"<GO>\",\"<PRED>\",\"<EOS>\",\"<PAD>\"]\n","if len(vocab) > 40000-len(special_tags):\n","    vocab = vocab[0:40000-len(special_tags)]\n","    \n","\n","vocab += special_tags \n","\n","vocab_dict = {word:i for i,word in enumerate(vocab)}\n","\n","embeddings = []\n","for word in vocab:\n","    embeddings.append(vocab2embd[word].tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VALX9apAYacZ"},"source":["# SHUFFLE\n","\n","import random\n","\n","texts_idx = [idx for idx in range(0,len(texts))]\n","random.shuffle(texts_idx)\n","\n","texts = [texts[idx] for idx in texts_idx]\n","summaries = [summaries[idx] for idx in texts_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VOKxtnd0YacZ","executionInfo":{"status":"ok","timestamp":1616560715317,"user_tz":-540,"elapsed":474,"user":{"displayName":"jaeyoung shin","photoUrl":"https://lh4.googleusercontent.com/-7ZQ3GuRm24k/AAAAAAAAAAI/AAAAAAAAAO0/eUnHI3HOU-w/s64/photo.jpg","userId":"04984017837842544937"}},"outputId":"aa54abb7-6b23-4576-ee17-ba74ba0ca805"},"source":["import random\n","\n","index = random.randint(0,len(texts)-1)\n","\n","print(\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index]))\n","print(\"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SAMPLE CLEANED & TOKENIZED TEXT: \n","\n","['this', 'loose', 'leaf', 'tea', 'makes', 'a', 'better', 'cup', 'than', 'the', 'bags', ',', 'and', 'it', 'lasts', 'longer', 'in', 'the', 'strainer', '(', 'more', 'cups', 'out', 'of', 'one', 'scoop', ')', '.']\n","\n","SAMPLE CLEANED & TOKENIZED SUMMARY: \n","\n","['fab', 'cup', 'of', 'tea']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OwnhBsghYaca"},"source":["train_len = int(.7*len(texts))\n","val_len = int(.2*len(texts))\n","\n","train_summaries = summaries[0:train_len]\n","train_texts = texts[0:train_len]\n","\n","val_summaries = summaries[train_len:val_len+train_len]\n","val_texts = texts[train_len:train_len+val_len]\n","\n","test_summaries = summaries[train_len+val_len:]\n","test_texts = texts[train_len+val_len:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkWtADTBYaca"},"source":["def bucket_and_batch(texts, summaries, batch_size=32):\n","    \n","    global vocab_dict\n","    vocab2idx = vocab_dict\n","    \n","    PAD = vocab2idx['<PAD>']\n","    EOS = vocab2idx['<EOS>']\n","    UNK = vocab2idx['<UNK>']\n","\n","    true_seq_lens = np.zeros((len(texts)), dtype=int)\n","    for i in range(len(texts)):\n","        true_seq_lens[i] = len(texts[i])\n","\n","    # sorted in descending order after flip\n","    sorted_by_len_indices = np.flip(np.argsort(true_seq_lens), 0)\n","\n","    sorted_texts = []\n","    sorted_summaries = []\n","\n","    for i in range(len(texts)):\n","        sorted_texts.append(texts[sorted_by_len_indices[i]])\n","        sorted_summaries.append(summaries[sorted_by_len_indices[i]])\n","\n","    i = 0\n","    batches_texts = []\n","    batches_summaries = []\n","    batches_true_seq_in_lens = []\n","    batches_true_seq_out_lens = []\n","\n","    while i < len(sorted_texts):  # 길이별 내림차순 정렬한 텍스트 길이가 작을 때\n","\n","        if i+batch_size > len(sorted_texts): \n","            batch_size = len(sorted_texts)-i\n","\n","        batch_texts = []\n","        batch_summaries = []\n","        batch_true_seq_in_lens = []\n","        batch_true_seq_out_lens = []\n","\n","        max_in_len = len(sorted_texts[i])\n","        max_out_len = max([len(sorted_summaries[j])+1 for j in range(i,i+batch_size)])\n","\n","        for j in range(i, i + batch_size):\n","\n","            text = sorted_texts[j]\n","            summary = sorted_summaries[j]\n","            \n","            text = [vocab2idx.get(word,UNK) for word in text]\n","            summary = [vocab2idx.get(word,UNK) for word in summary]\n","            \n","            init_in_len = len(text)\n","            init_out_len = len(summary)+1 # +1 for EOS\n","\n","            while len(text) < max_in_len:\n","                text.append(PAD)\n","                \n","            summary.append(EOS)\n","            \n","            while len(summary) < max_out_len:\n","                summary.append(PAD)\n","\n","            batch_summaries.append(summary)\n","            batch_texts.append(text)\n","            batch_true_seq_in_lens.append(init_in_len)\n","            batch_true_seq_out_lens.append(init_out_len)\n","\n","        #batch_texts = np.asarray(batch_texts, dtype=np.int32)\n","        #batch_summaries = np.asarray(batch_summaries, dtype=np.int32)\n","        #batch_true_seq_in_lens = np.asarray(batch_true_seq_in_lens, dtype=np.int32)\n","        #batch_true_seq_out_lens = np.asarray(batch_true_seq_out_lens, dtype=np.int32)\n","\n","        batches_texts.append(batch_texts)\n","        batches_summaries.append(batch_summaries)\n","        batches_true_seq_in_lens.append(batch_true_seq_in_lens)\n","        batches_true_seq_out_lens.append(batch_true_seq_out_lens)\n","\n","        i += batch_size\n","\n","    return batches_texts, batches_summaries, batches_true_seq_in_lens, batches_true_seq_out_lens\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3G0iP8VBYacb"},"source":["train_batches_x,train_batches_y,\\\n","train_batches_in_lens, train_batches_out_lens = bucket_and_batch(train_texts,train_summaries)\n","val_batches_x,val_batches_y,\\\n","val_batches_in_lens,val_batches_out_lens= bucket_and_batch(val_texts,val_summaries)\n","test_batches_x,test_batches_y,\\\n","test_batches_in_lens,test_batches_out_lens= bucket_and_batch(test_texts,test_summaries)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9_18zI9Yacb"},"source":["#Saving processed data in another file.\n","\n","import json\n","\n","diction = {}\n","diction['vocab']=vocab\n","diction['embd']=embeddings\n","diction['train_batches_x']=train_batches_x\n","diction['train_batches_y']=train_batches_y\n","diction['train_batches_in_len'] = train_batches_in_lens\n","diction['train_batches_out_len'] = train_batches_out_lens\n","diction['val_batches_x']=val_batches_x\n","diction['val_batches_y']=val_batches_y\n","diction['val_batches_in_len'] = val_batches_in_lens\n","diction['val_batches_out_len'] = val_batches_out_lens\n","diction['test_batches_x']=test_batches_x\n","diction['test_batches_y']=test_batches_y\n","diction['test_batches_in_len'] = test_batches_in_lens\n","diction['test_batches_out_len'] = test_batches_out_lens\n","\n","with open('ProcessedData.json', 'w') as fp:\n","    json.dump(diction, fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPeqq0p7Yacc"},"source":[""],"execution_count":null,"outputs":[]}]}