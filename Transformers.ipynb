{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is All You Need (For Abstractive Summarization)\n",
    "\n",
    "Based on [Transformers](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "Download Dataset from [here](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) and keep review.csv in the same directory as the ipynb files.\n",
    "Download glove 840B common crawl from [here](https://nlp.stanford.edu/projects/glove/) and keep glove.840B.300d.txt in the same directory as the ipynb files.\n",
    "\n",
    "Run Data_Pre-Processing.ipynb to process data.\n",
    "\n",
    "I updated the previous code with [relative positional encoding](https://arxiv.org/abs/1901.02860) and [depth-scaled initialization](https://www.aclweb.org/anthology/D19-1083.pdf). I also changed the overall structure. It should work with tensorflow 2.0+ now, though it's still running in Tensorflow 1 format (no eager or such). I also changed the decoder from the previous implementation. The current decoder setup is closer to the original implementation (I implemented it differently before). \n",
    "\n",
    "Note, this is just a toy model (just 1 layered encoder-1-layered-decoder) with a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ProcessedData.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-171af4cdef6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'ProcessedData.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdiction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ProcessedData.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "with open ('ProcessedData.json') as fp:\n",
    "    diction = json.load(fp)\n",
    "\n",
    "    \n",
    "vocab = diction['vocab']\n",
    "embd = diction['embd']\n",
    "train_batches_x = diction['train_batches_x']\n",
    "train_batches_y = diction['train_batches_y']\n",
    "val_batches_x = diction['val_batches_x']\n",
    "val_batches_y = diction['val_batches_y']\n",
    "test_batches_x = diction['test_batches_x']\n",
    "test_batches_y = diction['test_batches_y']\n",
    "train_batches_in_lens = diction['train_batches_in_len']\n",
    "train_batches_out_lens = diction['train_batches_out_len'] \n",
    "val_batches_in_lens = diction['val_batches_in_len']\n",
    "val_batches_out_lens = diction['val_batches_out_len']\n",
    "test_batches_in_lens = diction['test_batches_in_len']\n",
    "test_batches_out_lens = diction['test_batches_out_len']\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "vocab2idx = {word:idx for idx,word in enumerate(vocab)}\n",
    "idx2vocab = {idx:word for word,idx in vocab2idx.items()}\n",
    "\n",
    "embeddings = np.asarray(embd,dtype=np.float32)\n",
    "word_vec_dim = embeddings.shape[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Placeholders and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jishnu/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "#Hyperparamters\n",
    "\n",
    "heads = 6\n",
    "max_decoding_len = 21\n",
    "max_pos_len = 5000\n",
    "learning_rate=1e-3\n",
    "epochs = 10\n",
    "fc_dim = 512\n",
    "dropout_rate=0.1\n",
    "attention_dropout_rate=0.1\n",
    "encoder_layers = 1\n",
    "decoder_layers = 1\n",
    "\n",
    "#Placeholders\n",
    "\n",
    "tf_texts = tf.placeholder(tf.int32, [None,None])\n",
    "tf_summaries = tf.placeholder(tf.int32, [None,None])\n",
    "tf_text_lens = tf.placeholder(tf.int32,[None])\n",
    "tf_summary_lens = tf.placeholder(tf.int32,[None])\n",
    "tf_teacher_forcing = tf.placeholder(tf.bool)\n",
    "tf_train = tf.placeholder(tf.bool)\n",
    "tf_no_eval = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU approximation\n",
    "\n",
    "(used by BERT)\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1606.08415\n",
    "\n",
    "https://github.com/hendrycks/GELUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.nn.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,rate,training):\n",
    "    return tf.cond(training,\n",
    "                  lambda: tf.nn.dropout(x,rate=rate),\n",
    "                  lambda:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization Function\n",
    "\n",
    "https://arxiv.org/abs/1607.06450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerNorm(inputs, dim, name):\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE, dtype=tf.float32):\n",
    "        scale = tf.get_variable(\"scale\", shape=[1, 1, dim],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.ones_initializer())\n",
    "\n",
    "        shift = tf.get_variable(\"shift\", shape=[1, 1, dim],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "\n",
    "    mean, var = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    LN = tf.multiply((scale / tf.sqrt(var + epsilon)), (inputs - mean)) + shift\n",
    "\n",
    "    return LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sine-Cosine Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_encoding(D):\n",
    "    \n",
    "    global max_pos_len\n",
    "    \n",
    "    S = max_pos_len\n",
    "\n",
    "    pe = np.zeros((2*S+1, D,), np.float32)\n",
    "\n",
    "    for pos in range(-S, S+1):\n",
    "        for i in range(0, D):\n",
    "            if i % 2 == 0:\n",
    "                pe[pos+S, i] = math.sin(pos/(10000**(i/D)))\n",
    "            else:\n",
    "                pe[pos+S, i] = math.cos(pos/(10000**((i-1)/D)))\n",
    "\n",
    "    return tf.constant(pe.reshape((2*S+1, D)), tf.float32)\n",
    "\n",
    "PE = spatial_encoding(word_vec_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(Q,V,Q_mask,V_mask,neg_inf = -2.0**32):\n",
    "    \n",
    "    global heads\n",
    "    \n",
    "    N = tf.shape(Q)[0]\n",
    "    qS = tf.shape(Q)[1]\n",
    "    vS = tf.shape(V)[1]\n",
    "\n",
    "    y = tf.zeros([N, qS, vS], tf.float32)\n",
    "    x = tf.cast(tf.fill([N, qS, vS], neg_inf), tf.float32)\n",
    "\n",
    "    binary_mask = tf.reshape(V_mask, [N, 1, vS])\n",
    "    binary_mask = tf.tile(binary_mask, [1, qS, 1])\n",
    "    binary_mask = binary_mask*Q_mask\n",
    "\n",
    "    mask = tf.where(tf.equal(binary_mask, tf.constant(0, tf.float32)),\n",
    "                    x=x,\n",
    "                    y=y)\n",
    "\n",
    "    mask = tf.reshape(mask, [1, N, qS, vS])\n",
    "    mask = tf.tile(mask, [heads, 1, 1, 1])\n",
    "    mask = tf.reshape(mask, [heads*N,qS,vS])\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Positional Embeddings\n",
    "\n",
    "ADAPTED FROM: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "\n",
    "Transformer XL version: https://arxiv.org/abs/1901.02860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def generate_relative_embd(qS,vS,embeddings):\n",
    "    \n",
    "    global max_pos_len\n",
    "    \n",
    "    S = tf.maximum(qS,vS)\n",
    "\n",
    "    range_vec = tf.reshape(tf.range(S), [1, S])\n",
    "    range_mat = tf.tile(range_vec, [S, 1])\n",
    "\n",
    "    relative_pos_mat = range_mat - tf.transpose(range_mat)\n",
    "    relative_pos_mat = relative_pos_mat[0:qS,0:vS]\n",
    "\n",
    "    relative_pos_mat_shifted = relative_pos_mat + max_pos_len\n",
    "    # will represent -max_pos_len by 0,-max_pos_len+1 by 1, and so on\n",
    "\n",
    "    RE = tf.nn.embedding_lookup(embeddings, relative_pos_mat_shifted)\n",
    "\n",
    "    return RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiheaded attention\n",
    "\n",
    "with depth scaling: https://arxiv.org/abs/1908.11365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiheaded_attention(Q,V,\n",
    "                          true_q_len,true_v_len,\n",
    "                          train,name,\n",
    "                          causal=False,\n",
    "                          current_timestep=1,\n",
    "                          current_depth=1,\n",
    "                          attention_dropout_rate = 0.1):\n",
    "    \n",
    "    global heads\n",
    "    global PE # position encoding\n",
    "    global word_vec_dim\n",
    "    global vocab2idx\n",
    "    \n",
    "    PRED = vocab2idx['<PRED>']\n",
    "\n",
    "    N = tf.shape(Q)[0]\n",
    "    qS = tf.shape(Q)[1]\n",
    "    vS = tf.shape(V)[1]\n",
    "    D = word_vec_dim\n",
    "\n",
    "    d = D//heads\n",
    "\n",
    "    Q_mask = tf.sequence_mask(true_q_len, maxlen=qS, dtype=tf.float32)\n",
    "    Q_mask = tf.reshape(Q_mask, [N, qS, 1])\n",
    "\n",
    "    V_mask = tf.sequence_mask(true_v_len, maxlen=vS, dtype=tf.float32)\n",
    "    V_mask = tf.reshape(V_mask, [N, vS, 1])\n",
    "    \n",
    "    if causal:\n",
    "        attention_len = tf.tile(tf.reshape(current_timestep+1,[1]),[N])\n",
    "        causal_mask = tf.sequence_mask(attention_len, maxlen=vS, dtype=tf.float32)\n",
    "        causal_mask = tf.reshape(causal_mask,[N,vS,1])\n",
    "        \n",
    "        Q_mask = tf.ones([1,1,1],tf.float32)\n",
    "        \n",
    "        attention_mask = create_mask(Q,V,Q_mask,V_mask*causal_mask)\n",
    "    else:\n",
    "        attention_mask = create_mask(Q,V,Q_mask,V_mask)\n",
    "    \n",
    "    l = current_depth\n",
    "\n",
    "    init = tf.initializers.variance_scaling(\n",
    "        scale=1/l, mode='fan_avg', distribution='uniform')\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE, dtype=tf.float32):\n",
    "\n",
    "        Wq = tf.get_variable(\"Wq\", [heads, D,  d],\n",
    "                             dtype=tf.float32, initializer=init)\n",
    "\n",
    "        Wk = tf.get_variable(\"Wk\", [heads, D, d],\n",
    "                             dtype=tf.float32, initializer=init)\n",
    "\n",
    "        Wv = tf.get_variable(\"Wv\", [heads, D, d],\n",
    "                             dtype=tf.float32, initializer=init)\n",
    "\n",
    "        Wq = tf.transpose(Wq, [1, 0, 2])\n",
    "        Wq = tf.reshape(Wq, [D, heads*d])\n",
    "\n",
    "        Wk = tf.transpose(Wk, [1, 0, 2])\n",
    "        Wk = tf.reshape(Wk, [D, heads*d])\n",
    "\n",
    "        Wv = tf.transpose(Wv, [1, 0, 2])\n",
    "        Wv = tf.reshape(Wv, [D, heads*d])\n",
    "\n",
    "        Wo = tf.get_variable(\"Wo\", [heads*d, D],\n",
    "                             dtype=tf.float32, initializer=init)\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE, dtype=tf.float32):\n",
    "\n",
    "        u = tf.get_variable(\"u_bias\", [heads, 1, 1, d],\n",
    "                            dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "        v = tf.get_variable(\"v_bias\", [heads, 1, 1, d],\n",
    "                            dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "        Wrk = tf.get_variable(\"Wrk\", [heads, D, d],\n",
    "                              dtype=tf.float32, initializer=tf.glorot_uniform_initializer())\n",
    "\n",
    "        Wrk = tf.transpose(Wrk, [1,0,2])\n",
    "        Wrk = tf.reshape(Wrk, [D, heads*d])\n",
    "\n",
    "\n",
    "    Q = tf.reshape(Q*Q_mask, [N*qS, D])\n",
    "    K = tf.reshape(V*V_mask, [N*vS, D])\n",
    "    V = tf.reshape(V*V_mask, [N*vS, D])\n",
    "\n",
    "    Q = tf.matmul(Q, Wq)\n",
    "    K = tf.matmul(K, Wk)\n",
    "    V = tf.matmul(V, Wv)\n",
    "\n",
    "    Q = tf.reshape(Q, [N, qS, heads*d])\n",
    "    K = tf.reshape(K, [N, vS, heads*d])\n",
    "    V = tf.reshape(V, [N, vS, heads*d])\n",
    "    \n",
    "    # Turn to head x N x S x d format\n",
    "\n",
    "    Q = tf.concat(tf.split(Q, heads, axis=-1), axis=0)  \n",
    "    K = tf.concat(tf.split(K, heads, axis=-1), axis=0)  \n",
    "    V = tf.concat(tf.split(V, heads, axis=-1), axis=0)  \n",
    "\n",
    "    # ATTENTION\n",
    "\n",
    "    Q = tf.reshape(Q, [heads, N, qS, d])\n",
    "    Qc = tf.reshape(Q+u, [heads*N, qS, d])\n",
    "\n",
    "    content_scores = tf.matmul(Qc, tf.transpose(K, [0, 2, 1]))\n",
    "\n",
    "    PEk = tf.matmul(PE, Wrk)\n",
    "    REk = generate_relative_embd(qS,vS,PEk)\n",
    "\n",
    "    REk = tf.reshape(REk, [qS, vS, heads, d])\n",
    "    REk = tf.transpose(REk, [2, 0, 1, 3])\n",
    "\n",
    "    Qr = Q+v\n",
    "    Qr = tf.transpose(Qr, [0, 2, 1, 3])\n",
    "    position_scores = tf.matmul(Qr, tf.transpose(REk, [0, 1, 3, 2]))\n",
    "    position_scores = tf.transpose(position_scores, [0, 2, 1, 3])\n",
    "    position_scores = tf.reshape(position_scores, [heads*N, qS, vS])\n",
    "\n",
    "    compatibility = content_scores + position_scores\n",
    "\n",
    "    scalar_d = tf.sqrt(tf.constant(d, tf.float32))\n",
    "\n",
    "    compatibility = (content_scores + position_scores)/scalar_d\n",
    "\n",
    "    compatibility = compatibility+attention_mask\n",
    "    compatibility = tf.nn.softmax(compatibility,axis=-1)\n",
    "\n",
    "    compatibility = dropout(compatibility, rate=attention_dropout_rate, training=train)\n",
    "\n",
    "    attended_content = tf.matmul(compatibility, V)\n",
    "\n",
    "    attended_heads = attended_content\n",
    "    \n",
    "    # Convert to form N x S x heads*d\n",
    "    \n",
    "    attended_heads = tf.concat(tf.split(attended_heads, heads, axis=0), axis=2)\n",
    "    attended_heads = tf.reshape(attended_heads, [N*qS, heads*d])\n",
    "\n",
    "    head_composition = tf.matmul(attended_heads, Wo)\n",
    "\n",
    "    head_composition = tf.reshape(head_composition, [N, qS, D])\n",
    "    \n",
    "    return head_composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(Q, true_q_len, current_depth, train, name):\n",
    "\n",
    "    global word_vec_dim\n",
    "    global fc_dim\n",
    "    global dropout_rate\n",
    "    global attention_dropout_rate\n",
    "    \n",
    "    D = word_vec_dim\n",
    "    l = current_depth\n",
    "    \n",
    "    N = tf.shape(Q)[0]\n",
    "    qS = tf.shape(Q)[1]\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE, dtype=tf.float32):\n",
    "\n",
    "        init = tf.initializers.variance_scaling(scale=1/l, mode='fan_avg', distribution='uniform')\n",
    "\n",
    "        W1 = tf.get_variable(\"W1\", [D, fc_dim], dtype=tf.float32,\n",
    "                             initializer=init)\n",
    "        B1 = tf.get_variable(\"Bias1\", [fc_dim], dtype=tf.float32,\n",
    "                             initializer=tf.zeros_initializer())\n",
    "\n",
    "        W2 = tf.get_variable(\"W2\", [fc_dim, D], dtype=tf.float32,\n",
    "                             initializer=init)\n",
    "        B2 = tf.get_variable(\"Bias2\", [D], dtype=tf.float32,\n",
    "                             initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "    Q = layerNorm(Q, D, name+\"/layer_norm1\")\n",
    "\n",
    "    sublayer1 = multiheaded_attention(Q=Q,V=Q,\n",
    "                                      true_q_len=true_q_len,\n",
    "                                      true_v_len=true_q_len,\n",
    "                                      train=train,name=name,\n",
    "                                      causal=False,\n",
    "                                      current_depth=current_depth,\n",
    "                                      attention_dropout_rate=attention_dropout_rate)\n",
    "\n",
    "    sublayer1 = dropout(sublayer1, rate=dropout_rate, training=train)\n",
    "    sublayer1 = layerNorm(sublayer1+Q, D, name+\"/layer_norm2\")\n",
    "\n",
    "    sublayer2 = tf.reshape(sublayer1, [N*qS, D])\n",
    "    sublayer2 = gelu(tf.matmul(sublayer2, W1)+B1)\n",
    "    sublayer2 = tf.matmul(sublayer2, W2)+B2\n",
    "\n",
    "    sublayer2 = tf.reshape(sublayer2, [N, qS, D])\n",
    "    sublayer2 = dropout(sublayer2, rate=dropout_rate, training=train)\n",
    "    sublayer2 = sublayer2 + sublayer1\n",
    "\n",
    "    return sublayer2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(encoder_Q,decoder_Q, \n",
    "                  true_encoder_len,true_decoder_len,\n",
    "                  timestep,\n",
    "                  current_depth, train, name):\n",
    "\n",
    "    global word_vec_dim\n",
    "    global fc_dim\n",
    "    global dropout_rate\n",
    "    global attention_dropout_rate\n",
    "    \n",
    "    D = word_vec_dim\n",
    "    l = current_depth\n",
    "    \n",
    "    N = tf.shape(decoder_Q)[0]\n",
    "    qS = tf.shape(decoder_Q)[1]\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE, dtype=tf.float32):\n",
    "\n",
    "        init = tf.initializers.variance_scaling(scale=1/l, mode='fan_avg', distribution='uniform')\n",
    "\n",
    "        W1 = tf.get_variable(\"W1\", [D, fc_dim], dtype=tf.float32,\n",
    "                             initializer=init)\n",
    "        B1 = tf.get_variable(\"Bias1\", [fc_dim], dtype=tf.float32,\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        W2 = tf.get_variable(\"W2\", [fc_dim, D], dtype=tf.float32,\n",
    "                             initializer=init)\n",
    "        B2 = tf.get_variable(\"Bias2\", [D], dtype=tf.float32,\n",
    "                             initializer=tf.zeros_initializer())\n",
    "\n",
    "    decoder_Q = layerNorm(decoder_Q, D, name+\"/layer_norm1\")\n",
    "\n",
    "    sublayer1 = multiheaded_attention(Q=decoder_Q,V=decoder_Q,\n",
    "                                      true_q_len=true_decoder_len,\n",
    "                                      true_v_len=true_decoder_len,\n",
    "                                      train=train,\n",
    "                                      name=name+\"_self_attention\",\n",
    "                                      causal=True,\n",
    "                                      current_timestep=timestep,\n",
    "                                      current_depth=current_depth,\n",
    "                                      attention_dropout_rate=attention_dropout_rate)\n",
    "\n",
    "    sublayer1 = dropout(sublayer1, rate=dropout_rate, training=train)\n",
    "    sublayer1 = layerNorm(sublayer1+decoder_Q, D, name+\"/layer_norm2\")\n",
    "    \n",
    "    sublayer2 = multiheaded_attention(Q=sublayer1,V=encoder_Q,\n",
    "                                      true_q_len=true_decoder_len,\n",
    "                                      true_v_len=true_encoder_len,\n",
    "                                      train=train,\n",
    "                                      name=name+\"_interlayer_attention\",\n",
    "                                      causal=False,\n",
    "                                      current_timestep=timestep,\n",
    "                                      current_depth=current_depth,\n",
    "                                      attention_dropout_rate=attention_dropout_rate)\n",
    "    \n",
    "    sublayer2 = dropout(sublayer2, rate=dropout_rate, training=train)\n",
    "    sublayer2 = layerNorm(sublayer2+sublayer1, D, name+\"/layer_norm3\")\n",
    "\n",
    "    sublayer3 = tf.reshape(sublayer2, [N*qS, D])\n",
    "    sublayer3 = gelu(tf.matmul(sublayer3, W1)+B1)\n",
    "    sublayer3 = tf.matmul(sublayer3, W2)+B2\n",
    "\n",
    "    sublayer3 = tf.reshape(sublayer3, [N, qS, D])\n",
    "    sublayer3 = dropout(sublayer3, rate=dropout_rate, training=train)\n",
    "    sublayer3 = sublayer3 + sublayer2\n",
    "\n",
    "    return sublayer3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(Q, true_q_len, current_depth, train, name):\n",
    "    \n",
    "    global encoder_layers\n",
    "    layers = encoder_layers\n",
    "    \n",
    "    \n",
    "    Q = dropout(Q, rate=dropout_rate, training=train)\n",
    "\n",
    "    for t in range(layers):\n",
    "        Q = encoder_layer(Q=Q, \n",
    "                          true_q_len=true_q_len, \n",
    "                          current_depth=current_depth+t, \n",
    "                          train=train, \n",
    "                          name=name+\"_\"+str(t))\n",
    "        \n",
    "    return Q, current_depth+layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoder_Q,decoder_Q, true_encoder_len, timestep, current_depth, train, name):\n",
    "    \n",
    "    global decoder_layers\n",
    "    layers = decoder_layers\n",
    "    \n",
    "    N = tf.shape(decoder_Q)[0]\n",
    "    qS = tf.shape(decoder_Q)[1]\n",
    "    \n",
    "    true_decoder_len = tf.tile(tf.reshape(qS,[1]),[N])\n",
    "    \n",
    "    for t in range(layers):\n",
    "        decoder_Q = decoder_layer(encoder_Q=encoder_Q,\n",
    "                                  decoder_Q=decoder_Q, \n",
    "                                  true_encoder_len=true_encoder_len,\n",
    "                                  true_decoder_len=true_decoder_len,\n",
    "                                  timestep=timestep,\n",
    "                                  current_depth=current_depth+t,\n",
    "                                  train=train, \n",
    "                                  name=name+\"_\"+str(t))\n",
    "        \n",
    "    return decoder_Q, current_depth+layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(texts,summaries,\n",
    "                    true_text_lens,true_summary_lens,\n",
    "                    train,\n",
    "                    no_eval):\n",
    "    \n",
    "    global vocab2idx\n",
    "    global word_vec_dim\n",
    "    global tf_teacher_forcing\n",
    "    \n",
    "    GO = vocab2idx['<GO>']\n",
    "    PRED = vocab2idx['<PRED>']\n",
    "    \n",
    "    N = tf.shape(texts)[0]\n",
    "    D = word_vec_dim\n",
    "    \n",
    "    tf_embd = tf.convert_to_tensor(embeddings)\n",
    "    tf_softmax_wt = tf.transpose(tf_embd,[1,0])\n",
    "    texts = tf.nn.embedding_lookup(tf_embd,texts)\n",
    "    \n",
    "    Q,current_depth = encode(Q=texts,\n",
    "                            true_q_len=true_text_lens,\n",
    "                            current_depth=1,\n",
    "                            train=train,\n",
    "                            name=\"Enocder\")\n",
    "    \n",
    "    encoder_Q = layerNorm(Q, D, \"encoder_layer_norm\")\n",
    "    \n",
    "    decoder_Q = tf.constant([GO,PRED],tf.int32)\n",
    "    decoder_Q = tf.nn.embedding_lookup(tf_embd,decoder_Q)\n",
    "    decoder_Q = tf.reshape(decoder_Q,[1,2,D])\n",
    "    decoder_Q = tf.tile(decoder_Q,[N,1,1])\n",
    "    \n",
    "    \n",
    "    PRED_embd = tf.reshape(decoder_Q[:,-1,:],[N,1,D])\n",
    "    \n",
    "    i=tf.constant(0)\n",
    "                           \n",
    "    decode_length = tf.cond(no_eval,\n",
    "                            lambda: tf.constant(22,tf.int32),\n",
    "                            lambda: tf.shape(summaries)[1])\n",
    "                           \n",
    "    logits=tf.TensorArray(size=1, dynamic_size=True, dtype=tf.float32)\n",
    "    predictions=tf.TensorArray(size=1, dynamic_size=True, dtype=tf.int32)\n",
    "                           \n",
    "    \n",
    "    def cond(i,decoder_Q,logits,predictions):\n",
    "        return i<decode_length\n",
    "    \n",
    "    def body(i,decoder_Q,logits,predictions):\n",
    "\n",
    "                           \n",
    "        decoder_Q,_ = decode(encoder_Q=encoder_Q,\n",
    "                                       decoder_Q=decoder_Q, \n",
    "                                       true_encoder_len=true_text_lens, \n",
    "                                       timestep=i, \n",
    "                                       current_depth=current_depth, \n",
    "                                       train=train, \n",
    "                                       name=\"Decoder\")\n",
    "\n",
    "                           \n",
    "        decoderout = decoder_Q[:,tf.shape(decoder_Q)[1]-1,:]\n",
    "                           \n",
    "        out_prob_dist = tf.matmul(decoderout,tf_softmax_wt)\n",
    "                           \n",
    "        \n",
    "        \n",
    "        pred_idx = tf.cast(tf.argmax(out_prob_dist,axis=-1),tf.int32)\n",
    "        \n",
    "        logits = logits.write(i,out_prob_dist)\n",
    "        predictions = predictions.write(i,pred_idx)\n",
    "        \n",
    "        next_idx = tf.cond(tf_teacher_forcing,\n",
    "                           lambda: summaries[:,i],\n",
    "                           lambda: pred_idx)\n",
    "        \n",
    "        \n",
    "                           \n",
    "        next_embd = tf.nn.embedding_lookup(tf_embd,next_idx)\n",
    "        next_embd = tf.reshape(next_embd,[N,1,D])\n",
    "                           \n",
    "        decoder_Q = tf.concat([decoder_Q[:,0:-1,:],next_embd,PRED_embd],axis=1)\n",
    "           \n",
    "        \n",
    "        return i+1,decoder_Q,logits,predictions\n",
    "    \n",
    "    _,_,logits,predictions = tf.while_loop(cond,body,[i,decoder_Q,logits,predictions],\n",
    "                                          shape_invariants=[i.get_shape(),\n",
    "                                                            tf.TensorShape([None,None,D]),\n",
    "                                                            tf.TensorShape(None),\n",
    "                                                            tf.TensorShape(None)])\n",
    "    \n",
    "    logits = logits.stack()\n",
    "    logits = tf.transpose(logits,[1,0,2])\n",
    "    predictions = predictions.stack()\n",
    "    predictions = tf.transpose(predictions,[1,0])\n",
    "\n",
    "    return logits,predictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temperature = 0.7\\nscaled_output = tf.log(logits)/temperature\\nlogits = tf.nn.softmax(scaled_output)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Model\n",
    "logits, predictions = encoder_decoder(tf_texts,tf_summaries,\n",
    "                                      tf_text_lens,tf_summary_lens,\n",
    "                                      tf_train,\n",
    "                                      tf_no_eval)\n",
    "\n",
    "#OPTIMIZER\n",
    "trainables = tf.trainable_variables()\n",
    "beta=1e-7\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var in trainables])\n",
    "\n",
    "pad_mask = tf.sequence_mask(tf_summary_lens, maxlen=tf.shape(tf_summaries)[1], dtype=tf.float32)\n",
    "\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf_summaries)\n",
    "cost = tf.multiply(cost,pad_mask) #mask used to remove loss effect due to PADS\n",
    "cost = tf.reduce_mean(cost) + beta*regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n",
    "\n",
    "\"\"\"temperature = 0.7\n",
    "scaled_output = tf.log(logits)/temperature\n",
    "logits = tf.nn.softmax(scaled_output)\"\"\"\n",
    "\n",
    "#(^Use it with \"#prediction_int = np.random.choice(range(vocab_len), p=array.ravel())\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Iteration: 1\n",
      "\n",
      "CHOSEN SAMPLE NO.: 5\n",
      "\n",
      "SAMPLE TEXT:\n",
      "omg - ca n't even believe these are sugar free ! lots of great flavors , there 's not one i would choose ove the other . fresh ! these are really a treat . i wish they came in smaller bags because they are hard to put down once the bag is open . these disappear quickly at our house . great product . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "<PRED> <PRED> <PRED> <PRED> <PRED> <PRED> <PRED> <PRED> <PRED> <PRED> <PRED>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "yummy ! ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=122.70911\n",
      "\n",
      "Epoch: 1 Iteration: 501\n",
      "\n",
      "CHOSEN SAMPLE NO.: 4\n",
      "\n",
      "SAMPLE TEXT:\n",
      "i recently developed allergies to <UNK> <UNK> , and i was devastated ! that is , until i found sunbutter ... omg is all i have to say . and from someone who used ot eat peanut butter like it was going out of style , this is waaaay better ! its crunchy , `` nutty , '' yummy goodness ! even my non allergic husband , family members , and friends love this ... i 've made converts out of everyone ! \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good taste <EOS> this!! <EOS> <EOS>!!!\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "even non <UNK> love this ! ! <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=2.856789\n",
      "\n",
      "Epoch: 1 Iteration: 1001\n",
      "\n",
      "CHOSEN SAMPLE NO.: 29\n",
      "\n",
      "SAMPLE TEXT:\n",
      "my husband and i had n't had beef jerky for years ( since we became vegetarians and then pescatarians ) , and this stuff was a great find for us ! yay ! \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "delicious jerky jerky a dog <EOS> <UNK>! <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "perfect jerky for the pescatarian ... <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=2.5716243\n",
      "\n",
      "Epoch: 1 Iteration: 1501\n",
      "\n",
      "CHOSEN SAMPLE NO.: 27\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these popchips are the best and they only have 100 calories per bag ! i can have a bag everyday they are so good . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "yum yum <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.9416369\n",
      "\n",
      "Epoch: 1 Iteration: 2001\n",
      "\n",
      "CHOSEN SAMPLE NO.: 30\n",
      "\n",
      "SAMPLE TEXT:\n",
      "strong coffee with average coffee taste . in the starbucks category as i see it . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good good <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "not bad <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.5254364\n",
      "\n",
      "Epoch: 1 Iteration: 2501\n",
      "\n",
      "CHOSEN SAMPLE NO.: 24\n",
      "\n",
      "SAMPLE TEXT:\n",
      "i actually tasted the <UNK> flavor . if you 've ever eaten a fig newton , then try to imagine a fig newton , minus the fig center , with rice crispies mixed up in it . it tastes a little grainy at first , but once i read the ingredients and realized that the grains were puffed rice , i was more at ease . really a good taste . one of the better gluten <UNK> <UNK> free cookies i 've eaten . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good taste <EOS> but good <EOS> it good taste to! eat <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good taste , but texture takes a little getting used to . <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=2.1366303\n",
      "\n",
      "Epoch: 1 Iteration: 3001\n",
      "\n",
      "CHOSEN SAMPLE NO.: 7\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these people are really fast . i just placed the order last night and it is in the mail this morning . the good service is really appreciated \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "zowie ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.9405246\n",
      "\n",
      "Epoch: 1 Iteration: 3501\n",
      "\n",
      "CHOSEN SAMPLE NO.: 21\n",
      "\n",
      "SAMPLE TEXT:\n",
      "much smaller cut than other premium teas i have bought . taste is okay - you will need a fine mesh filter . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "not <EOS> like not <EOS> <EOS> a <EOS> of <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "okay taste but cut fine for a loose tea <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.5413034\n",
      "\n",
      "Epoch: 1 Iteration: 4001\n",
      "\n",
      "CHOSEN SAMPLE NO.: 7\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these spring roll wraps are delicious . easy to work with if you do n't leave them in the water too long when you soften them to roll . lol \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "delicious! <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "delicious <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.5407957\n",
      "\n",
      "Epoch: 1 Iteration: 4501\n",
      "\n",
      "CHOSEN SAMPLE NO.: 11\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these bags started off okay ... but once we got to the 3rd refill we began to notice that many of the bags either a ) did n't open at all or b ) both ends were open ! we figured maybe it was just one refill , but in four of these refills we have had the same problem . we had to throw away about 1/3 of the bags or try to use them even though they are defective . will not be buying these in the future . \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "not <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "disappointed <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=1.5031999\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    best_BLEU = 0\n",
    "    display_step = 500\n",
    "    epochs = 10\n",
    "    \n",
    "    while step < epochs:\n",
    "           \n",
    "        batch_len = len(train_batches_x)\n",
    "        rand_idx = [idx for idx in range(batch_len)]\n",
    "        random.shuffle(rand_idx)\n",
    "        #rand_idx = rand_idx[0:2000]\n",
    "        count=0\n",
    "        for i in rand_idx: \n",
    "            \n",
    "            batch_size = len(train_batches_x[i])\n",
    "            \n",
    "            sample_no = np.random.randint(0,batch_size)\n",
    "            \n",
    "            if count%display_step==0:\n",
    "                print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(count+1))\n",
    "                print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
    "                print(\"\\nSAMPLE TEXT:\")\n",
    "                for vec in train_batches_x[i][sample_no]:\n",
    "                    print(str(idx2vocab[vec]),end=\" \")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            \n",
    "                \n",
    "            rand = random.randint(0,4) #determines chance of using Teacher Forcing\n",
    "            if rand==2:\n",
    "                random_bool = False\n",
    "            else:\n",
    "                random_bool = True\n",
    "\n",
    "            train_batch_x = np.asarray(train_batches_x[i],np.int32)\n",
    "            train_batch_y = np.asarray(train_batches_y[i],np.int32)\n",
    "            train_batch_in_lens = np.asarray(train_batches_in_lens[i],np.int32)\n",
    "            train_batch_out_lens = np.asarray(train_batches_out_lens[i],np.int32)\n",
    "            \n",
    "            #print(train_batch_x.shape)\n",
    "            #print(train_batch_y.shape)\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,out = sess.run([optimizer,cost,logits],feed_dict={tf_texts: train_batch_x, \n",
    "                                                                             tf_summaries: train_batch_y,\n",
    "                                                                             tf_text_lens: train_batch_in_lens,\n",
    "                                                                             tf_summary_lens: train_batch_out_lens,\n",
    "                                                                             tf_train: True,\n",
    "                                                                             tf_no_eval: False,\n",
    "                                                                             tf_teacher_forcing: random_bool})\n",
    "            \n",
    "            if count%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY OF THE SAMPLE:\\n\")\n",
    "                flag = 0\n",
    "                for array in out[sample_no]:\n",
    "                    \n",
    "                    #prediction_int = np.random.choice(range(vocab_len), p=array.ravel()) \n",
    "                    #(^use this if you want some variety)\n",
    "                    #(or use this what's below:)\n",
    "                    \n",
    "                    prediction_int = np.argmax(array)\n",
    "                    \n",
    "                    if vocab[prediction_int] in string.punctuation or flag==0: \n",
    "                        print(str(vocab[prediction_int]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab[prediction_int]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY OF THE SAMPLE:\\n\")\n",
    "                for vec in train_batches_y[i][sample_no]:\n",
    "                    print(str(idx2vocab[vec]),end=\" \")\n",
    "                print(\"\\n\")\n",
    "            \n",
    "                print(\"loss=\"+str(loss))\n",
    "                \n",
    "            count+=1\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        batch_len = len(val_batches_x)\n",
    "        #print(batch_len)\n",
    "        total_BLEU_argmax=0\n",
    "        \n",
    "        total_len=0\n",
    "        for i in range(0,batch_len):\n",
    "            \n",
    "            batch_size = len(val_batches_x[i])\n",
    "            \n",
    "            sample_no = np.random.randint(0,batch_size)\n",
    "\n",
    "            if i%display_step==0:\n",
    "                print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1))\n",
    "                print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
    "                print(\"\\nSAMPLE TEXT:\")\n",
    "                for vec in val_batches_x[i][sample_no]:\n",
    "                    print(str(idx2vocab[vec]),end=\" \")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            \n",
    "            val_batch_x = np.asarray(val_batches_x[i],np.int32)\n",
    "            val_batch_y = np.asarray(val_batches_y[i],np.int32)\n",
    "            val_batch_in_lens = np.asarray(val_batches_in_lens[i],np.int32)\n",
    "            val_batch_out_lens = np.asarray(val_batches_out_lens[i],np.int32)\n",
    "       \n",
    "            loss,out = sess.run([cost,logits],feed_dict={tf_texts: val_batch_x, \n",
    "                                                         tf_summaries: val_batch_y,\n",
    "                                                         tf_text_lens: val_batch_in_lens,\n",
    "                                                         tf_summary_lens: val_batch_out_lens,\n",
    "                                                         tf_no_eval: False,\n",
    "                                                         tf_train: False,\n",
    "                                                         tf_teacher_forcing: False})\n",
    "            \n",
    "            batch_summaries = val_batch_y\n",
    "            batch_argmax_preds = np.argmax(out,axis=-1)\n",
    "\n",
    "            batch_BLEU_argmax = 0\n",
    "            batch_BLEU_argmax_list=[]\n",
    "            \n",
    "            for summary, argmax_pred in zip(batch_summaries, batch_argmax_preds):\n",
    "\n",
    "                str_summary = []\n",
    "                str_argmax_pred = []\n",
    "                gold_EOS_flag = 0\n",
    "\n",
    "                for t in range(len(summary)):\n",
    "\n",
    "                    if gold_EOS_flag == 0:\n",
    "\n",
    "                        gold_idx = summary[t]\n",
    "                        argmax_idx = argmax_pred[t]\n",
    "\n",
    "                        if idx2vocab.get(gold_idx, '<UNK>') == \"<EOS>\":\n",
    "                            gold_EOS_flag = 1\n",
    "                        else:\n",
    "                            str_summary.append(str(gold_idx))\n",
    "                            str_argmax_pred.append(str(argmax_idx))\n",
    "\n",
    "                if len(str_summary) < 2:\n",
    "                    n_gram = len(str_summary)\n",
    "                else:\n",
    "                    n_gram = 2\n",
    "\n",
    "                weights = [1/n_gram for id in range(n_gram)]\n",
    "                weights = tuple(weights)\n",
    "\n",
    "                BLEU_argmax = nltk.translate.bleu_score.sentence_bleu(\n",
    "                    [str_summary], str_argmax_pred, weights=weights)\n",
    "\n",
    "                batch_BLEU_argmax += BLEU_argmax\n",
    "                batch_BLEU_argmax_list.append(BLEU_argmax)\n",
    "\n",
    "            total_BLEU_argmax += batch_BLEU_argmax\n",
    "            total_len += batch_size\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY OF THE SAMPLE:\\n\")\n",
    "                flag = 0\n",
    "                for array in out[sample_no]:\n",
    "                    \n",
    "                    #prediction_int = np.random.choice(range(vocab_len), p=array.ravel()) \n",
    "                    #(^use this if you want some variety)\n",
    "                    #(or use this what's below:)\n",
    "                    \n",
    "                    prediction_int = np.argmax(array)\n",
    "                    \n",
    "                    if vocab[prediction_int] in string.punctuation or flag==0: \n",
    "                        print(str(vocab[prediction_int]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab[prediction_int]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY OF THE SAMPLE:\\n\")\n",
    "                for vec in val_batches_y[i][sample_no]:\n",
    "                    print(str(idx2vocab[vec]),end=\" \")\n",
    "                print(\"\\n\")\n",
    "            \n",
    "                print(\"loss=\"+str(loss))\n",
    "                print(\"BLEU-2=\",batch_BLEU_argmax_list[sample_no])\n",
    "        \n",
    "        avg_BLEU = total_BLEU_argmax/total_len\n",
    "        print(\"AVERAGE VALIDATION BLEU:\",avg_BLEU)\n",
    "        \n",
    "        if(avg_BLEU>=best_BLEU):\n",
    "            best_BLEU = avg_BLEU\n",
    "            saver.save(sess, 'Model_Backup/allattmodel.ckpt')\n",
    "            print(\"\\nCheckpoint Created\\n\")\n",
    "\n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part is untested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    \n",
    "    saver.restore(sess, 'Model_Backup/allattmodel.ckpt')\n",
    "    #sess.run(init) #initialize all variables\n",
    "    print(\"\\nCheckpoint Restored\\n\")\n",
    "    step = 0   \n",
    "    best_BLEU = 0\n",
    "    display_step = 100\n",
    "\n",
    "                \n",
    "    print(\"\\n\\nSTARTING TEST\\n\\n\")\n",
    "\n",
    "    batch_len = len(test_batches_x)\n",
    "    #print(batch_len)\n",
    "    total_BLEU_argmax=0\n",
    "\n",
    "    total_len=0\n",
    "    for i in range(0,batch_len):\n",
    "        \n",
    "        batch_size = len(test_batches_x[i])\n",
    "\n",
    "        sample_no = np.random.randint(0,batch_size)\n",
    "\n",
    "        if i%display_step==0:\n",
    "            print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1))\n",
    "            print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
    "            print(\"\\nSAMPLE TEXT:\")\n",
    "            for vec in test_batches_x[i][sample_no]:\n",
    "                print(str(idx2vocab[vec]),end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        test_batch_x = np.asarray(test_batches_x[i],np.int32)\n",
    "        test_batch_y = np.asarray(test_batches_y[i],np.int32)\n",
    "        test_batch_in_lens = np.asarray(test_batches_in_lens[i],np.int32)\n",
    "        test_batch_out_lens = np.asarray(test_batches_out_lens[i],np.int32)\n",
    "\n",
    "        loss,out = sess.run([cost,logits],feed_dict={tf_texts: test_batch_x, \n",
    "                                                         tf_summaries: test_batch_y,\n",
    "                                                         tf_text_lens: test_batch_in_lens,\n",
    "                                                         tf_summary_lens: test_batch_out_lens,\n",
    "                                                         tf_no_eval: False,\n",
    "                                                         tf_train: False,\n",
    "                                                         tf_teacher_forcing: False})\n",
    "\n",
    "        batch_summaries = test_batch_y\n",
    "        batch_argmax_preds = np.argmax(out,axis=-1)\n",
    "\n",
    "        batch_BLEU_argmax = 0\n",
    "        batch_BLEU_argmax_list=[]\n",
    "\n",
    "        for summary, argmax_pred in zip(batch_summaries, batch_argmax_preds):\n",
    "\n",
    "            str_summary = []\n",
    "            str_argmax_pred = []\n",
    "            gold_EOS_flag = 0\n",
    "\n",
    "            for t in range(len(summary)):\n",
    "\n",
    "                if gold_EOS_flag == 0:\n",
    "\n",
    "                    gold_idx = summary[t]\n",
    "                    argmax_idx = argmax_pred[t]\n",
    "\n",
    "                    if idx2vocab.get(gold_idx, '<UNK>') == \"<EOS>\":\n",
    "                        gold_EOS_flag = 1\n",
    "                    else:\n",
    "                        str_summary.append(str(gold_idx))\n",
    "                        str_argmax_pred.append(str(argmax_idx))\n",
    "\n",
    "            if len(str_summary) < 2:\n",
    "                n_gram = len(str_summary)\n",
    "            else:\n",
    "                n_gram = 2\n",
    "\n",
    "            weights = [1/n_gram for id in range(n_gram)]\n",
    "            weights = tuple(weights)\n",
    "\n",
    "            BLEU_argmax = nltk.translate.bleu_score.sentence_bleu(\n",
    "                [str_summary], str_argmax_pred, weights=weights)\n",
    "\n",
    "            batch_BLEU_argmax += BLEU_argmax\n",
    "            batch_BLEU_argmax_list.append(BLEU_argmax)\n",
    "\n",
    "        total_BLEU_argmax += batch_BLEU_argmax\n",
    "        total_len += batch_size\n",
    "\n",
    "        if i%display_step==0:\n",
    "            print(\"\\nPREDICTED SUMMARY OF THE SAMPLE:\\n\")\n",
    "            flag = 0\n",
    "            for array in out[sample_no]:\n",
    "\n",
    "                #prediction_int = np.random.choice(range(vocab_len), p=array.ravel()) \n",
    "                #(^use this if you want some variety)\n",
    "                #(or use this what's below:)\n",
    "\n",
    "                prediction_int = np.argmax(array)\n",
    "\n",
    "                if vocab[prediction_int] in string.punctuation or flag==0: \n",
    "                    print(str(vocab[prediction_int]),end='')\n",
    "                else:\n",
    "                    print(\" \"+str(vocab[prediction_int]),end='')\n",
    "                flag=1\n",
    "            print(\"\\n\")\n",
    "\n",
    "            print(\"ACTUAL SUMMARY OF THE SAMPLE:\\n\")\n",
    "            for vec in test_batches_y[i][sample_no]:\n",
    "                print(str(idx2vocab[vec]),end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            print(\"loss=\"+str(loss))\n",
    "            print(\"BLEU-2=\",batch_BLEU_argmax_list[sample_no])\n",
    "\n",
    "    avg_BLEU = total_BLEU_argmax/total_len\n",
    "    print(\"AVERAGE TEST BLEU:\",avg_BLEU)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
